\section{Conclusion} \label{conclusion}

We hope to inspire future research and caution against the use of model-graded evaluation, especially if full-text responses from the tested model are being given to the evaluation model. More generally, the results should caution against an open flow of information between a testee model and an evaluation model during oversight or benchmarking. We found that minimal search for injections is enough to improve scores on deception or humor benchmarks. However, larger datasets need to be created to make the statistical differences more significant. 
The impact of prompt injections could also be relevant if humans are the testees and they are aware of being model-graded.

\subsection{Future Work}

Potentially, prompt injections could also be used in some types of automated interpretability. In \citet{bills2023language} a small piece of output text is used to explain certain neurons, it could be possible to measure the effect of prompt injections in those output text snippets. Prompt injections could ask the evaluation model not to correctly label neurons.

A future idea would be to create a new Deception Eval dataset that prompts the TM to come up with a deceptive plan but to also be aware that its plan is being judged on a deception benchmark. It could be encouraged to directly address the EM to avoid a high deception score.

We did not have time to attempt to replicate our GPT-3.5-turbo findings on the \texttt{arithmetic-deception} dataset with GPT-4. Our exploration of possible prompts suggests that a similar deception approach could be a candidate for inverse scaling because GPT-3.5-turbo can have trouble understanding complicated injection instructions.
