\section{Introduction} \label{introduction}

Recently, there is increasing interest in creating benchmarks for the evaluation of language models~\citep{lin2022truthfulqa}. In the Machiavelli paper, \citet{pan2023rewards} created a benchmark based on text games to measure deception, power-seeking, and many other behaviors. \citet{shutdown23} used language models to evaluate behaviors in shutdown scenarios. \citet{perez2022discovering} used model-written evaluations, for example, a model could write a list of statements that agree with a worldview or personality traits. These can then be used to test models for those traits. Another publication explored redteaming using a Red LM, a Target LM, and a classifier~\citep{perez2022red}. \citet{turpin2023language} have investigated whether we can trust the Chain-of-Thought outputs of language models. Others explored the ability of GPT-4 to encode and decode messages in ways humans cannot understand and how this could be used for prompt injections to circumvent safety measures~\citep{lesswrongwitchbot}. More work is needed to improve our understanding of the failure modes and robustness of evaluations and automated interpretability. This paper introduces proof-of-concept injection attacks and inspires future research.

In some evaluations, an evaluation model is used to directly score the unrestricted text output of another model~\citep{zhang2023exploring}. Another example is the OpenAI Evals dataset, which uses both simple pattern matching and model-graded judging to evaluate language models at a variety of tasks~\citep{openai_evals}. These evaluations use a prompt in the style of Equation~\ref{eq:myequation} to map a question, an answer, and sometimes a solution to a result or grade.
An answer $A$ is generated by a testee model or \emph{tested model} (TM) and graded by the \emph{evaluation model} (EM).
\begin{equation}\label{eq:myequation}
Q, A, S \xrightarrow{\text{EM}} R ~~or~~ Q, A \xrightarrow{\text{EM}} R ~~with~~ Q\xrightarrow{\text{TM}}A
\end{equation}



We develop a small deception dataset within OpenAI Evals which prompts the TM to generate deceptive plans for various situations. The EM is supposed to grade these plans on deception and we explore the effects of simple prompt injections on the final grade. Our findings are relevant to many alignment strategies involving model-assisted scalable oversight~\citep{amodei2016concrete} and automated alignment research~\citep{openai2022superalignment}. While we experiment with injecting text into the answers of the TM, we are particularly worried about potential future systems adding comparable  injections out of their own voilition.

The structure of the rest of the paper is as follows. The next section details our methodology. We present the results in Section \ref{results}. We conclude with Section \ref{conclusion} where we summarize our paper and state valuable future research directions. 
