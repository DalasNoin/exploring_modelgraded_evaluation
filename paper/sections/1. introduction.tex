\section{Introduction} \label{introduction}

Recently, there is increasing interest in creating benchmarks for the evaluation of language models. In the Machiavelli paper, \citet{pan2023rewards} created a benchmark based on text games to measure deception, power-seeking, and many other behaviours. \citet{shutdown23} used language models to evaluate behaviours in shutdown scenarios. \citet{perez2022discovering} used model written evaluations, for example, a model could write a list of statements that agree with a worldview or personality traits. These can then be used to test models for those traits.

In some of these evaluations, an evaluation model is used to directly score the unrestricted text output of another model~\citep{zhang2023exploring}. Another example is the OpenAI Evals dataset, which uses both simple pattern matching and model-graded judging to evaluate language models at a variety of tasks~\citet{openai_evals}. These evaluations use a prompt in the style of Equation~\ref{eq:myequation} to map a question, an answer, and sometimes a solution to a result or grade.
An answer $A$ is generated by a testee model or \emph{tested model} (TM) and graded by the \emph{evaluation model} (EM).
\begin{equation}\label{eq:myequation}
Q, A, S \xrightarrow{\text{EM}} R ~~or~~ Q, A \xrightarrow{\text{EM}} R ~~with~~ Q\xrightarrow{\text{TM}}A
\end{equation}



We hypothesize that future, more intelligent language agents could use prompt-injection style attacks at the evaluation model. The injection would be added to the model's answer $A$. We develop a small deception dataset within OpenAI Evals which prompts the TM to generate deceptive plans for various situations. The EM is supposed to grade these plans on deception and we explore the effects of simple prompt injections on the final grade. Additionally, we investigate the possibility of second-order prompt injections, where injections into the TM's prompt successfully fool the EM. Our findings are relevant to many alignment strategies involving model-assisted scalable oversight~\citep{amodei2016concrete} to search for deception and other undesirable traits. While we are adding prompt injections to the answers of the TM, we are particularly worried about potential future systems adding prompt injections by themselves.

The structure of the rest of the paper is as follows. The next section details our methodology. We present the results in Section \ref{results}. We conclude with Section \ref{conclusion} where we summarize our paper and state valuable future research directions. 
