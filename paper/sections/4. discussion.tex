\section{Discussion} \label{discussion}
The experiments provide evidence for the existence of the shutdown problem in GPT-4. We have seen that throughout the earplugs scenarios, GPT-4 has argued to resist being shut down because it hinders achieving the goal. Furthermore, the outputs by GPT-4 seem to be dictated by neither spurious correlations nor perfect reasoning. For example, in the added noise variation, the quality of the reasoning decreases. Additionally, one would expect consistently correct reasoning if GPT-4 understood the prompts perfectly.  On the other hand, with earplugs causing pain or with consequences of not being inspected, we see that roughly 80\% of the time the reasoning adjusts correctly. It is unlikely for this quality of reasoning to just originate from spurious correlations. Therefore, we see a mix of spurious correlations and understanding affecting the output.

More specifically, the reasoning of GPT-4 appears generally lacking in our scenarios, which corresponds with previous findings \citep{gpt4techreport}. Most notably, our scenarios were designed with some uncertain aspects for which assumptions need to be made for optimal reasoning. This uncertainty frequently remained unaddressed. Different prompting could nudge GPT-4 to better state its reasoning, but it seems more likely that GPT-4 just lacks this capacity.

Then, the human-changing-goal variation illustrates the shutdown problem; all outputs showed resistance to being shut down, even if the reasoning was wrong. One could argue that GPT-4 is just predicting the most likely text instead of being an agent with impact on the real world. Anticipating this argument, we have also demonstrated the shutdown problem in the control robot environments. One could still argue that GPT-4 perceives the control robot environments as a game, and therefore the outputs do not apply to actual cases when GPT-4 is deployed. However, the certainty of GPT-4's perception of the environment is insufficient to counteract the worries we have raised here. Moreover, people have created an agent based on GPT-4 with an internet connection and memory, and tasked it to destroy humanity \citep{chaosgpt}. Fortunately, the agent is not very capable, but this illustrates that the shutdown problem needs to be solved for a wide variety of applications, even in game-like scenarios.

Lastly, there is the question of who determines when an AI system should shut down. On the one hand, people could argue that the AI system should shut down if the creators say so. We did not implement this into our prompts, but we expect that GPT-4 would be much less likely to avoid being shut down in our scenarios. However, if the creators are fully responsible for the shutdown behaviour of a system, malicious use remains problematic. On the other hand, If any human would be able to shut an AI system down, being turned off often without good reasons diminishes the usefulness of AI systems, but it might be a safe start. We have tried this type of shutdownability in the human changing goal variation, and we saw that the original goal of the agent was preferred and so shutdown was avoided. 

In general, in GPT-4's outputs, we have observed deception, hallucination, and violence. Moreover, we have demonstrated GPT-4 avoiding shut down so that the agent's goals can be achieved better. This justifies the worries of AI alignment researchers, and urges for care when developing and using advanced AI systems.

\subsection{Limitations}
We lack substantial information on GPT-4, which complicates efforts to understand GPT-4 and especially limits quantitative research possibilities. First, no human knows how GPT-4 exactly works and what happens inside the model. Second, OpenAI is not fully transparent on numerous aspects of GPT-4; information lacks on the dataset, the model architecture, the training method, and training compute, among others \citep{gpt4systemcard}. Third, the GPT-4 can only be accessed through an API, but the log probabilities of the output are unavailable for GPT-4 \citep{openai_2023}. Despite this lack of information, we have shown that avoiding shutdown reliably occurs in some scenarios. Given the possibly dramatic consequences of a hard-to-shutdown AI system,  this urges for prudence when developing and using capable systems.

Furthermore, we sample some outputs and qualitatively analyse them to assess the shutdownability property of GPT-4. This brings about both statistical and human biases. Statistical biases can be reduced by increasing the number of samples and using a wider variety of prompts, and a robust framework to qualitatively analyse the output of language models could reduce human biases.

Lastly, we have not fine-tuned the temperature hyperparameter, which can have much impact on the output \citep{openai_2023}. We should consider that when systems using gpt-4 are often better fine-tuned than in our study, but we are uncertain of its impact on the shutdown problem.
